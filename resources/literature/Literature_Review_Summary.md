[1]: https://arxiv.org/pdf/1811.07497.pdf
[2]: https://www.researchgate.net/publication/331258862_Geotagging_Tweets_to_Landmarks_using_Convolutional_Neural_Networks_with_Text_and_Posting_Time
[3]: https://link-springer-com.ezp.lib.unimelb.edu.au/content/pdf/10.1007%2Fs12065-018-0163-3.pdf
[4]: http://shura.shu.ac.uk/23724/8/Bhowmik_content_aware_tweet%20%28AM%29.pdf
[5]: https://pdf-sciencedirectassets-com.ezp.lib.unimelb.edu.au/271653/1-s2.0-S0167923619X00032/1-s2.0-S0167923619300442/main.pdf?x-amz-security-token=AgoJb3JpZ2luX2VjEO3%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIQC7up88dEqt6vmp%2FnLdlIumh7Ap2261YShfRixLlwAx%2FwIgPUAqgozhKYQEXskTbTfiB8Q3jBNYIz05gtX6SBHVdHoq4wMI5f%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FARACGgwwNTkwMDM1NDY4NjUiDD%2BWMgukj%2B0Lw7WsRiq3A3v5Lb4pxTC0j7wgpoRBu384aWh0ZsrozeJSwK1beQQuexrPOqCdaRKAiQ3qCV00kWFMmmU%2BeQ74gJh5ZOZLgAhHFMeOcSFWsg%2FsMRgfluBhLkn9%2Bg8idONA9UFhdrNuU%2B6EY6wjGzwyhmT1nIhfb7fOqWnp3ee0su6MZ9VRHPwBkiSi%2FWBmoYsFCgPTon1Di99okDnq5UkHC23UoCbe1t3Q4Vxqez1iZBsqV9x0D%2BNQTRy8BLRQ83Jf3GVyOiJ00Y0WQVK2iRBA6JEjHbXY8e%2FlalaC40Q4IHTBQ9kpT3OC7oDsEvNgr6HnzmwGve7uIWvTDsaVQRCy6p0CQnE1ygUlXjYj98bjej3B4p3he86TFLaIb6Jd7x2g%2Bc3XNTKg6vriwB0askQ6gF7luxWYIb7%2F7KSLXLFtGZGII3JLquijo7MkUbl4DNB6xW3qQWJimCA5ewZmCZuLJUbSs50vW2IH28uIQYLIqPnMpUYMNLBNxb%2FyfubpMNw%2FiPKbHPuZoPKolkdqcK5Md%2Biylyecy5%2BnmDF6PUlgWlnQOcFfTTxcHcfjipB8feOIaco5%2FUv4OQh1kpi5qlUwuqbu5gU6tAGET7Sz89nbM4bOyJYRFiJ7X2j6DqSVfaj9L7DRkHVOj9jTgaEQrZ6ijqwwBfNaFy0RBlW4zchbr4CEO6rioOI56G%2F%2F7XzwMhvxVlZroPW03U5WE%2FlKWMjkqxVklLiMqolJnzGxGTR8lF%2F1eMpFiqyViUPyN%2FyJFiLbMQHvgmGhxmOzGAIYA1t021mdoa3Edj9EsgNkwfqqqmZWqTqPYnbHa0If1UnAiy0sQNRMqkYdVn4zYgA%3D&AWSAccessKeyId=ASIAQ3PHCVTYTU5VEUXH&Expires=1557900158&Signature=SWoSIx3VHzfEvY1W1Z17b0%2FDeQQ%3D&hash=fac68e8be7eccc02f111db94ee8c2451f755453fcf969e8c99b54337d59c8bcf&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0167923619300442&tid=spdf-d75fd1f0-6d31-4649-ac06-f83400cb73fb&sid=2410e5969e3b674fbe8803788a7ccd4de9a8gxrqa&type=client
[6]: https://arxiv.org/pdf/1805.04612.pdf
[7]: http://delivery.acm.org.ezp.lib.unimelb.edu.au/10.1145/3180000/3178112/a34-bakerman.pdf?ip=128.250.144.144&id=3178112&acc=ACTIVE%20SERVICE&key=65D80644F295BC0D%2E157276E45169F5BA%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&__acm__=1557917552_a66b21cd811e464d99e4895226d72d72
[8]: https://www.aclweb.org/anthology/W18-6102
[9]: https://link-springer-com.ezp.lib.unimelb.edu.au/article/10.1007/s11276-018-01897-1
[10]: https://link.springer.com/chapter/10.1007/978-3-319-73706-5_21
[11]: https://arxiv.org/pdf/1704.04008.pdf
[12]: https://arxiv.org/pdf/1804.08049.pdf
[13]: https://arxiv.org/pdf/1708.04358.pdf
[14]: https://www.aclweb.org/anthology/P17-1116
[15]: https://www.aclweb.org/anthology/K18-1005
[Stanford tokenizer]: https://nlp.stanford.edu/pubs/StanfordCoreNlp2014.pdf
[GloVe]: https://nlp.stanford.edu/projects/glove/
[Gensim]: https://radimrehurek.com/gensim/
[SMART]: https://www.lextek.com/manuals/onix/stopwords2.html
[TIME]: http://techland.time.com/2009/06/08/the-500-most-frequently-used-words-on-twitter/
[Stemmer]: https://www.tutorialspoint.com/python/python_stemming_algorithms.htm
[NER]: https://pythonprogramming.net/named-entity-recognition-stanford-ner-tagger/
[DBSCAN]: http://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html



## Literature Review of Project 2 COMP30027

| Literature | Resources                                                    | Preprocessing                                                | Feature Engineering                                          | Models                                                       |
| ---------- | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| [1], 2018  | 1. Blogs: <br />a. text (**equally distributed**)<br />2. Tweets: text (**equally distributed**) | 1. **Source Filtering**: <br />a. **Blogs**: <br />i. filter out group blog<br />ii. profiles have cumulative chars less than 600.<br />iii. Clean of HTML tags<br />iv. Tokenization [Stanford tokenizer]<br />b. **Tweets**:<br />i. remove retweets<br />ii. exclude hashtags and mentions<br />iii. only keep user has total length 600 characters.<br />2. **Word Filtering**<br />all: exclude rare words (appear in less than 3 users) | **1. IGR (Information Gain Ratio)**: bad<br />**2. WLH(Word Locality Heuristic)**: basis<br />**3. Location Lexicons**: use, 3 rule: <br />a. at least used by *p*(T: 11, B: 500) users; <br />b. WLH score above certain threshold *h*(T: 16, B: 17); <br />c. each lexicons contains at least *t*(T: 2, B: 3) words. | **BaseLine**: NB with all words, Blogs 9.3%, Tweets 28.53%<br />**1. Multinomial Naive Bayes**: <br />b. T: WLH(30%): 57.47%<br />**2. SVM (linear)**: <br />a. B: WLH(60%): 32.72% |
| [2], 2019  | Tweets in MEL: context, metadata                             |                                                              | **Text**: word with length (*n* = 50), each ith word is represented as an **embedding vector** of dimension d, for tweets less than n words, zero-padded to length n<br />**Tools**: counted based: [GloVe] (dim = 200)<br />**Metadata**: one-hot encodings | CNN: 67.85% - 89.19%                                         |
| [3], 2018  | Tweets: content, user activity, user network                 | remove URL, generic hashtag, special character, repeated character, tweet normalization | Content: Weigthed Matrix                                     | Content: Simple Algor based on matrix                        |
| [4], 2018  | Tweets: content                                              | remove duplicate messages                                    | Prediction based **word embedding**, <br />**Tools**: word2vec<br />Steps: <br />1. linear vector calculation<br />2. synonyms identification<br />3. similarity threshold determination (cosine similarity + Jaccard similarity)<br /> **Quadtree Data Partitioning** | Content: <br />1. Logistic Regression / Maximum Entropy: **BETTER** 43.70% - 60.24%<br />2. Random Forest:<br />3. DT:<br />4. ANN: <br />5: MNB: **BETTER** 30.20% - 43.76% |
| [6], 2018  | Tweets: content, network, posting time                       |                                                              | 1. **TF-IDF** (Term Frequency - Inverse Document Frequency): a document a concatenation of tweets posted from the same user.<br />2. **Context Feature**: doc2vec (used algorithm: PV-DBOW(Distributed Bag of Words of Paragraph Vector))<br />3. **Library**: [Gensim] | MENET(Multi-Entry Neural Network)                            |
| [7], 2018  | Tweets: content                                              | 1. Remove all special characters, emojis, # and punctuation except @.<br />2. reduce all chars to lowercase.<br />3. Remove all stop words.([SMART] stop word list, 500 most common Twitter words from [TIME])<br />4. Spell check (replace according to Jaccard coefficient for strings)<br />5. Porter stemming algorithm [Stemmer] | Unigram: unigram variables (latitude × longitude coordinates) | Hybrid Model (a series of **GMMs**)                          |
| [9], 2019  | Tweets: content                                              | 0. Remove @-mention, hashtag(#) and hyperlink starting with ‘http’ symbol<br />1. Extract named entities (ORGANIZATION/LOCATION): Stanford [NER] tagger<br />2. Remove ambiguous or duplicated entities: Stanford [NER] tagger<br /><br />*Note*: NER > TF-IDF and removing stop words<br />2.1: remove named entities with characters < 4, and with frequency less than threshold ε = 10<br />3. named entities: density-based spatial clustering algorithm [DBSCAN] (*eps* = 0.5, *min_samples* = 5) (DBSCAN > K-means since not require predefined K)<br />4: From result of 3, remove all GLOBAL entities and LOCAL entites that are far away from the density-connected clusters.<br /> | 5. construct vector                                          | MRM (Multilayer Recognition Model)**<br />MNB** with smoothing (replace Laplace 1 with a)(54.82%) [5]: good summary of previous work |
| [10], 2019 | Tweets, content and metadata                                 | Lower casing, **Tool**: whitespace tokenizer                 | word embeddings (batch normalization)                        | **LSTM**                                                     |
| [15], 2018 | Tweets, content and metadata                                 |                                                              | word embedding + bidirectional LSTM (BiLSTM)                 | **LSTM**                                                     |

5 Types of categories: Text Approaches, Network Approaches, Hybird Approaches, Graph Convolutional Networks, Metadata-based Approach [7], [8], [9], [14]
Other interesting NN study: [11], [12], [13]